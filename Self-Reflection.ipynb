{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2453405e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"self_reflection\")\n",
    "\n",
    "import torch\n",
    "from datasets import Dataset, load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from ragas import evaluate\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.metrics import FactualCorrectness\n",
    "\n",
    "from CTRLEval.ctrleval import CTRLEval\n",
    "from evaluate.sent_similarity import Sent_Similar\n",
    "from loop_utils import main_loop\n",
    "from loop import knowledge_loop, response_loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b320f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"rungalileo/ragbench\", \"hotpotqa\", split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f5168b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds[0]['question']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3121b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ctrleval_scorer = CTRLEval(\n",
    "    iwf_dir=\"self_reflection/CTRLEval/iwf_full.txt\",\n",
    "    prompt_dir=\"self_reflection/CTRLEval/prompt/prompt_topic.txt\",\n",
    "    verbal_dir=\"self_reflection/CTRLEval/prompt/verbal_topic.txt\",\n",
    "    device='cuda',\n",
    ")\n",
    "entailment_scorer = Sent_Similar()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d76278d",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"Qwen/Qwen3-4B-Instruct-2507\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, quantization_config=bnb_config, device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3b0b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    continue_generate = False\n",
    "    no_number = False\n",
    "    no_aspect = False\n",
    "\n",
    "    max_loop = 1\n",
    "    max_knowledge_loop = 1\n",
    "    max_response_loop = 1\n",
    "    demo_num = 0\n",
    "\n",
    "    threshold_entailment = 0.8\n",
    "    threshold_fact = -1\n",
    "    threshold_consistency = -5\n",
    "\n",
    "    max_sample = 3000\n",
    "    temperature = 1.0\n",
    "    top_p = 1\n",
    "    top_k = 1\n",
    "    num_beams = 1\n",
    "    max_new_tokens = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb8bbbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Args()\n",
    "args.max_loop = 3\n",
    "args.max_knowledge_loop = 3\n",
    "args.max_response_loop = 3\n",
    "args.demo_num = 0\n",
    "args.threshold_entailment = 0.8\n",
    "args.threshold_fact = -1.0\n",
    "args.threshold_consistency = -5\n",
    "args.max_new_tokens = 256\n",
    "\n",
    "#final_knowledge, final_response, all_history_knowledge, all_history_response = main_loop(args, ds[0], model, tokenizer, knowledge_loop, response_loop, entailment_scorer, ctrleval_scorer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d36170",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(final_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5360ae60",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = []\n",
    "for d in ds.select(range(100)):\n",
    "    question = d[\"question\"]\n",
    "    reference = d[\"response\"]\n",
    "\n",
    "    _, final_response, _, _ = main_loop(args, d, model, tokenizer, knowledge_loop, response_loop, entailment_scorer, ctrleval_scorer)\n",
    "\n",
    "    dataset.append(\n",
    "        {\n",
    "            \"user_input\": question,\n",
    "            \"response\": final_response,\n",
    "            \"reference\": reference,\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daab644a",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    ")\n",
    "evaluator_llm = LangchainLLMWrapper(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4edc92d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_dataset = Dataset.from_list(dataset)\n",
    "\n",
    "ragas_result = evaluate(evaluation_dataset, metrics=[FactualCorrectness()], llm=evaluator_llm)\n",
    "print(ragas_result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI6130",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
