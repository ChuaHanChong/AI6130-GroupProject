{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2453405e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"self_reflection\")\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(\".env\")\n",
    "\n",
    "from typing import List, Tuple, Union\n",
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "import torch\n",
    "from datasets import Dataset, load_dataset\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from ragas import evaluate\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.metrics import FactualCorrectness\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "from CTRLEval.ctrleval import CTRLEval\n",
    "from evaluate.loop_eval_utils import evaluate_knowledge, evaluate_response\n",
    "from evaluate.sent_similarity import Sent_Similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b320f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_name = \"hotpotqa\"\n",
    "#ds_name = \"pubmedqa\"\n",
    "#ds_name = \"delucionqa\"\n",
    "ds = load_dataset(\"rungalileo/ragbench\", ds_name, split=\"test\")\n",
    "print(len(ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3121b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ctrleval_scorer = CTRLEval(\n",
    "    iwf_dir=\"self_reflection/CTRLEval/iwf_full.txt\",\n",
    "    prompt_dir=\"self_reflection/CTRLEval/prompt/prompt_topic.txt\",\n",
    "    verbal_dir=\"self_reflection/CTRLEval/prompt/verbal_topic.txt\",\n",
    "    device='cuda',\n",
    ")\n",
    "\n",
    "# Error: Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-xsum and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
    "# Ignore because Pegasus uses static, sinusoidal position embeddings (rather than learned embeddings) for both encoder and decoder.\n",
    "\n",
    "entailment_scorer = Sent_Similar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d76278d",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"Qwen/Qwen3-4B-Instruct-2507\"\n",
    "#MODEL_NAME = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, device_map=\"cuda\", dtype=torch.bfloat16).eval()\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3b0b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    continue_generate = False\n",
    "    no_number = False\n",
    "    no_aspect = False\n",
    "\n",
    "    max_loop = 1\n",
    "    max_knowledge_loop = 1\n",
    "    max_response_loop = 1\n",
    "    demo_num = 0\n",
    "\n",
    "    threshold_entailment = 0.8\n",
    "    threshold_fact = -1\n",
    "    threshold_consistency = -5\n",
    "\n",
    "    max_sample = 3000\n",
    "    temperature = 1.0\n",
    "    top_p = 0.90\n",
    "    top_k = 50\n",
    "    num_beams = 1\n",
    "    max_new_tokens = 128\n",
    "    repetition_penalty = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb8bbbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Args()\n",
    "args.max_loop = 3\n",
    "args.max_knowledge_loop = 3\n",
    "args.max_response_loop = 3\n",
    "args.demo_num = 0\n",
    "args.threshold_entailment = 0.8\n",
    "args.threshold_fact = -1.0\n",
    "args.threshold_consistency = -5\n",
    "args.max_new_tokens = 512\n",
    "args.repetition_penalty = 1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7d4a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_step(\n",
    "    model: AutoModelForCausalLM,\n",
    "    tokenizer: AutoTokenizer,\n",
    "    messages: List[dict[str, str]],\n",
    "    max_new_tokens: int = 512,\n",
    "    temperature: float = 0.2,\n",
    "    top_p: float = 0.9,\n",
    "    top_k: int = 50,\n",
    "    num_beams: int = 1,\n",
    "    repetition_penalty: float = 1.1,\n",
    ") -> str:\n",
    "    formatted_chat = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    inputs = tokenizer(formatted_chat, return_tensors=\"pt\", add_special_tokens=False).to(model.device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=temperature > 0,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            top_k=top_k,\n",
    "            num_beams=num_beams,\n",
    "            repetition_penalty=repetition_penalty,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    return tokenizer.decode(outputs[0][inputs[\"input_ids\"].size(1) :], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "def knowledge_loop(\n",
    "    args: Args,\n",
    "    question: str,\n",
    ") -> Tuple[str, List[Tuple[int, str, float]]]:\n",
    "    print(\"knowledge_loop\")\n",
    "\n",
    "    THRESHOLD_FACTUAL = args.threshold_fact\n",
    "    MAX_KNOWLEDGE_LOOP = args.max_knowledge_loop\n",
    "\n",
    "    candidates = []\n",
    "    history = []\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"\"\"You are an AI language model designed to provide accurate, relevant, and comprehensive background knowledge based on the given question.\"\"\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Provide background knowledge in one paragraph (approximately 400 words) to answer the question: \\\"{question}\\\".\",\n",
    "        },\n",
    "    ]\n",
    "    knowledge = generate_step(\n",
    "        model, \n",
    "        tokenizer, \n",
    "        messages,\n",
    "        max_new_tokens=args.max_new_tokens,\n",
    "        temperature=args.temperature,\n",
    "        top_p=args.top_p,\n",
    "        top_k=args.top_k,\n",
    "        num_beams=args.num_beams,\n",
    "        repetition_penalty=args.repetition_penalty,\n",
    "    )\n",
    "\n",
    "    loop_i = 0\n",
    "    if MAX_KNOWLEDGE_LOOP > 1:\n",
    "        factuality_score = evaluate_knowledge(model, args.demo_num, question, knowledge, tokenizer)\n",
    "        candidates.append([factuality_score, knowledge])\n",
    "        history.append([loop_i, knowledge, factuality_score])\n",
    "\n",
    "    loop_i += 1\n",
    "    while (loop_i < MAX_KNOWLEDGE_LOOP) and factuality_score < THRESHOLD_FACTUAL:\n",
    "        if args.no_aspect:\n",
    "            instruction = f\"Please refine the knowledge.\"\n",
    "        elif args.no_number:\n",
    "            instruction = f\"The knowledge is not strongly supported by empirical evidence. Please refine the knowledge to improve its factuality.\"\n",
    "        else:\n",
    "            instruction = f\"The factuality score for the knowledge is {factuality_score} less than {THRESHOLD_FACTUAL}, which means the knowledge is not strongly supported by empirical evidence. Please refine the knowledge to improve its factuality.\"\n",
    "\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are an AI language model designed to provide accurate, relevant, and comprehensive background knowledge based on the given question.\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f'The provided background knowledge for the question: \"{question}\" is \"{knowledge}\".\\n\\n{instruction}\\nThe refined knowledge should be in one paragraph (approximately 400 words).',\n",
    "            },\n",
    "        ]\n",
    "        knowledge = generate_step(\n",
    "            model, \n",
    "            tokenizer, \n",
    "            messages,\n",
    "            max_new_tokens=args.max_new_tokens,\n",
    "            temperature=args.temperature,\n",
    "            top_p=args.top_p,\n",
    "            top_k=args.top_k,\n",
    "            num_beams=args.num_beams,\n",
    "            repetition_penalty=args.repetition_penalty,\n",
    "        )\n",
    "\n",
    "        factuality_score = evaluate_knowledge(model, args.demo_num, question, knowledge, tokenizer)\n",
    "\n",
    "        candidates.append([factuality_score, knowledge])\n",
    "        history.append([loop_i, knowledge, factuality_score])\n",
    "        loop_i += 1\n",
    "\n",
    "    if (MAX_KNOWLEDGE_LOOP > 1) and factuality_score < THRESHOLD_FACTUAL:\n",
    "        candidates.sort()\n",
    "        return candidates[-1][-1], history\n",
    "    else:\n",
    "        return knowledge, history\n",
    "\n",
    "\n",
    "def response_loop(\n",
    "    args: Args,\n",
    "    question: str,\n",
    "    final_knowledge: str,\n",
    ") -> Tuple[str, List[Tuple[int, str, float, float]], float]:\n",
    "    print(\"response_loop\")\n",
    "\n",
    "    THRESHOLD_CONS = args.threshold_consistency\n",
    "    MAX_RESPONSE_LOOP = args.max_response_loop\n",
    "\n",
    "    candidates = []\n",
    "    entailment_score_question_list = []\n",
    "    history = []\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are an AI language model designed to provide accurate, relevant, and comprehensive answers to questions based on the given background knowledge.\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Refer to the background knowledge: \\\"{final_knowledge}\\\" and answer the question: \\\"{question}\\\" in one paragraph.\",\n",
    "        },\n",
    "    ]\n",
    "    response = generate_step(\n",
    "        model, \n",
    "        tokenizer, \n",
    "        messages,\n",
    "        max_new_tokens=args.max_new_tokens,\n",
    "        temperature=args.temperature,\n",
    "        top_p=args.top_p,\n",
    "        top_k=args.top_k,\n",
    "        num_beams=args.num_beams,\n",
    "        repetition_penalty=args.repetition_penalty,\n",
    "    )\n",
    "\n",
    "    loop_i = 0\n",
    "    if MAX_RESPONSE_LOOP > 1:\n",
    "        entailment_score_question, cons_score_knowledge = evaluate_response(entailment_scorer, ctrleval_scorer, question, response, final_knowledge)\n",
    "        candidates.append([(entailment_score_question + cons_score_knowledge) / 2, response])\n",
    "        entailment_score_question_list.append(entailment_score_question)\n",
    "        history.append([loop_i, response, entailment_score_question, cons_score_knowledge])\n",
    "\n",
    "    loop_i += 1\n",
    "    while loop_i < MAX_RESPONSE_LOOP and cons_score_knowledge < THRESHOLD_CONS:\n",
    "        if args.no_aspect:\n",
    "            instruction = f\"Please refine the response.\"\n",
    "        elif args.no_number:\n",
    "            instruction = f\"The alignment and consistency between response and knowledge are low. Please refine the response to improve its consistency.\"\n",
    "        else:\n",
    "            instruction = f\"The consistency score for the response is {cons_score_knowledge} less than {THRESHOLD_CONS}, which means the alignment and consistency between response and knowledge are low. Please refine the response to improve its consistency.\"\n",
    "\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are an AI language model designed to provide accurate, relevant, and comprehensive answers to questions based on the given background knowledge.\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\", \n",
    "                \"content\": f\"The generated response for the question: \\\"{question}\\\" is \\\"{response}\\\" based on the background knowledge: \\\"{final_knowledge}\\\".\\n\\n{instruction}\\nThe refined response should be in one paragraph.\",\n",
    "            },\n",
    "        ]\n",
    "        response = generate_step(\n",
    "            model, \n",
    "            tokenizer, \n",
    "            messages,\n",
    "            max_new_tokens=args.max_new_tokens,\n",
    "            temperature=args.temperature,\n",
    "            top_p=args.top_p,\n",
    "            top_k=args.top_k,\n",
    "            num_beams=args.num_beams,\n",
    "            repetition_penalty=args.repetition_penalty,\n",
    "        )\n",
    "\n",
    "        entailment_score_question, cons_score_knowledge = evaluate_response(entailment_scorer, ctrleval_scorer, question, response, final_knowledge)\n",
    "        candidates.append([(entailment_score_question + cons_score_knowledge) / 2, response])\n",
    "        entailment_score_question_list.append(entailment_score_question)\n",
    "        history.append([loop_i, response, entailment_score_question, cons_score_knowledge])\n",
    "\n",
    "        loop_i += 1\n",
    "\n",
    "    if MAX_RESPONSE_LOOP > 1 and cons_score_knowledge < THRESHOLD_CONS:\n",
    "        merge = zip(candidates, entailment_score_question_list)\n",
    "        merge = sorted(merge)\n",
    "        candidates, entailment_score_question_list = zip(*merge)\n",
    "        return candidates[-1][-1], history, entailment_score_question_list[-1]\n",
    "    else:\n",
    "        return response, history, entailment_score_question\n",
    "\n",
    "\n",
    "def reflection_loop(\n",
    "    args: Args,\n",
    "    question: str,\n",
    ") -> Tuple[str, str, List[Tuple[int, str, float]], List[Tuple[int, str, float, float]]]:\n",
    "    all_history_knowledge, all_history_response = [], []\n",
    "\n",
    "    THRESHOLD_ENTAIL = args.threshold_entailment\n",
    "    MAX_LOOP = args.max_loop\n",
    "\n",
    "    candidates = []\n",
    "    main_loop_i = 0\n",
    "    print(f\"main_loop {main_loop_i}\")\n",
    "\n",
    "    final_knowledge, history_knowledge = knowledge_loop(args, question)\n",
    "    all_history_knowledge += history_knowledge\n",
    "\n",
    "    final_response, history_response, entailment_score_question = response_loop(args, question, final_knowledge)\n",
    "    all_history_response += history_response\n",
    "    candidates.append([entailment_score_question, final_knowledge, final_response])\n",
    "\n",
    "    main_loop_i += 1\n",
    "    while main_loop_i < MAX_LOOP and entailment_score_question < THRESHOLD_ENTAIL:\n",
    "        print(f\"main_loop {main_loop_i}\")\n",
    "        final_knowledge, history_knowledge = knowledge_loop(args, question)\n",
    "        all_history_knowledge += history_knowledge\n",
    "\n",
    "        final_response, history_response, entailment_score_question = response_loop(args, question, final_knowledge)\n",
    "        all_history_response += history_response\n",
    "        candidates.append([entailment_score_question, final_knowledge, final_response])\n",
    "        main_loop_i += 1\n",
    "\n",
    "    if (MAX_LOOP > 1) and entailment_score_question < THRESHOLD_ENTAIL:\n",
    "        candidates.sort()\n",
    "        final_knowledge, final_response = candidates[-1][1:]\n",
    "\n",
    "    return final_knowledge, final_response, all_history_knowledge, all_history_response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5360ae60",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 100\n",
    "dataset = []\n",
    "for d in tqdm(ds.select(range(num_samples)), total=num_samples):\n",
    "    question = d[\"question\"]\n",
    "    reference = d[\"response\"]\n",
    "\n",
    "    _, final_response, _, _ = reflection_loop(args, question)\n",
    "\n",
    "    dataset.append(\n",
    "        {\n",
    "            \"user_input\": question,\n",
    "            \"response\": final_response,\n",
    "            \"reference\": reference,\n",
    "        }\n",
    "    )\n",
    "\n",
    "output_dir = \"results/exp-0\"\n",
    "with open(os.path.join(output_dir, f\"{MODEL_NAME.replace('/', '~')}_{ds_name}_responses_self-reflection.json\"), \"w\") as f:\n",
    "    json.dump(dataset, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daab644a",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator_llm = LangchainLLMWrapper(\n",
    "    ChatGoogleGenerativeAI(\n",
    "        model=\"gemini-2.5-flash-lite\",\n",
    "        temperature=0,\n",
    "        max_tokens=None,\n",
    "        timeout=None,\n",
    "        max_retries=4,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4edc92d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_dataset = Dataset.from_list(dataset)\n",
    "\n",
    "ragas_result = evaluate(evaluation_dataset, metrics=[FactualCorrectness()], llm=evaluator_llm)\n",
    "print(ragas_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1093f20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ragas_result_df = ragas_result.to_pandas()\n",
    "ragas_result_df.to_csv(os.path.join(output_dir, f\"{MODEL_NAME.replace('/', '~')}_{ds_name}_ragas-results_self-reflection.csv\"), index=False)\n",
    "#ragas_result_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI6130",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
