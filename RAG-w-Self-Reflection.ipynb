{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2453405e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2647483/1271306692.py:15: UserWarning: \n",
      "********************************************************************************\n",
      "RAGatouille WARNING: Future Release Notice\n",
      "--------------------------------------------\n",
      "RAGatouille version 0.0.10 will be migrating to a PyLate backend \n",
      "instead of the current Stanford ColBERT backend.\n",
      "PyLate is a fully mature, feature-equivalent backend, that greatly facilitates compatibility.\n",
      "However, please pin version <0.0.10 if you require the Stanford ColBERT backend.\n",
      "********************************************************************************\n",
      "  from ragatouille import RAGPretrainedModel\n",
      "/data/hanchong/miniconda3/envs/AI6130/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'validate_default' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'validate_default' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv(\".env\")\n",
    "\n",
    "from typing import List, Tuple\n",
    "\n",
    "import torch\n",
    "from datasets import Dataset, load_dataset\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_community.vectorstores.utils import DistanceStrategy\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from ragas import evaluate\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.metrics import ContextRelevance, FactualCorrectness, Faithfulness\n",
    "from ragatouille import RAGPretrainedModel\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    pipeline,\n",
    ")\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"self_reflection\")\n",
    "\n",
    "from CTRLEval.ctrleval import CTRLEval\n",
    "from evaluate.loop_eval_utils import evaluate_response\n",
    "from evaluate.sent_similarity import Sent_Similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3121b3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-large and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4456ad50f68043d6a586e4e85b3bb31e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "/data/hanchong/miniconda3/envs/AI6130/lib/python3.12/site-packages/colbert/utils/amp.py:12: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler()\n"
     ]
    }
   ],
   "source": [
    "ctrleval_scorer = CTRLEval(\n",
    "    iwf_dir=\"self_reflection/CTRLEval/iwf_full.txt\",\n",
    "    prompt_dir=\"self_reflection/CTRLEval/prompt/prompt_topic.txt\",\n",
    "    verbal_dir=\"self_reflection/CTRLEval/prompt/verbal_topic.txt\",\n",
    "    device=\"cuda\",\n",
    ")\n",
    "\n",
    "# Error: Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-xsum and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
    "# Ignore because Pegasus uses static, sinusoidal position embeddings (rather than learned embeddings) for both encoder and decoder.\n",
    "\n",
    "entailment_scorer = Sent_Similar()\n",
    "\n",
    "MODEL_NAME = \"Qwen/Qwen3-4B-Instruct-2507\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, quantization_config=bnb_config, device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "llm = pipeline(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    task=\"text-generation\",\n",
    "    do_sample=True,\n",
    "    temperature=0.2,\n",
    "    return_full_text=False,\n",
    "    max_new_tokens=500,\n",
    ")\n",
    "\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"Qwen/Qwen3-Embedding-0.6B\",\n",
    "    model_kwargs={\"device\": \"cuda\"},\n",
    "    encode_kwargs={\"normalize_embeddings\": True},\n",
    ")\n",
    "\n",
    "reranker = RAGPretrainedModel.from_pretrained(\"colbert-ir/colbertv2.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b320f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"rungalileo/ragbench\", \"hotpotqa\", split=\"test\")\n",
    "\n",
    "RAW_KNOWLEDGE_BASE = []\n",
    "for d in ds:\n",
    "    for doc in d[\"documents\"]:\n",
    "        RAW_KNOWLEDGE_BASE.append(doc)\n",
    "\n",
    "KNOWLEDGE_VECTOR_DATABASE = FAISS.from_texts(list(set(RAW_KNOWLEDGE_BASE)), embedding_model, distance_strategy=DistanceStrategy.COSINE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74ecc83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    no_number = False\n",
    "    no_aspect = False\n",
    "\n",
    "    max_loop = 1\n",
    "    max_response_loop = 1\n",
    "    demo_num = 0\n",
    "\n",
    "    threshold_entailment = 0.8\n",
    "    threshold_consistency = -5\n",
    "\n",
    "    temperature = 1.0\n",
    "    top_p = 1\n",
    "    top_k = 1\n",
    "    num_beams = 1\n",
    "    max_new_tokens = 128\n",
    "    repetition_penalty = 1.0\n",
    "\n",
    "\n",
    "args = Args()\n",
    "args.max_loop = 3\n",
    "args.max_response_loop = 3\n",
    "args.demo_num = 0\n",
    "args.threshold_entailment = 0.8\n",
    "args.threshold_consistency = -5\n",
    "args.max_new_tokens = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6497b381",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_step(\n",
    "    args: Args,\n",
    "    model: AutoModelForCausalLM,\n",
    "    tokenizer: AutoTokenizer,\n",
    "    messages: List[dict[str, str]],\n",
    ") -> str:\n",
    "    formatted_chat = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    inputs = tokenizer(formatted_chat, return_tensors=\"pt\", add_special_tokens=False).to(model.device)\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=args.max_new_tokens,\n",
    "        do_sample=args.temperature > 0,\n",
    "        temperature=args.temperature,\n",
    "        top_p=args.top_p,\n",
    "        top_k=args.top_k,\n",
    "        num_beams=args.num_beams,\n",
    "        repetition_penalty=args.repetition_penalty,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "    return tokenizer.decode(outputs[0][inputs[\"input_ids\"].size(1) :], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "def response_loop(\n",
    "    args: Args,\n",
    "    question: str,\n",
    "    final_knowledge: str,\n",
    ") -> Tuple[str, List[Tuple[int, str, float, float]], float]:\n",
    "    print(\"response_loop\")\n",
    "\n",
    "    THRESHOLD_CONS = args.threshold_consistency\n",
    "    MAX_RESPONSE_LOOP = args.max_response_loop\n",
    "\n",
    "    candidates = []\n",
    "    entailment_score_question_list = []\n",
    "    history = []\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are an AI language model designed to provide accurate, relevant, and comprehensive answers to questions based on the given background context.\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Answer the question: \\\"{question}\\\" with one paragraph by referring to the background context: \\n{final_knowledge}\",\n",
    "        },\n",
    "    ]\n",
    "    response = generate_step(args, model, tokenizer, messages)\n",
    "\n",
    "    loop_i = 0\n",
    "    if MAX_RESPONSE_LOOP > 1:\n",
    "        entailment_score_question, cons_score_knowledge = evaluate_response(entailment_scorer, ctrleval_scorer, question, response, final_knowledge)\n",
    "        candidates.append([(entailment_score_question + cons_score_knowledge) / 2, response])\n",
    "        entailment_score_question_list.append(entailment_score_question)\n",
    "        history.append([loop_i, response, entailment_score_question, cons_score_knowledge])\n",
    "\n",
    "    loop_i += 1\n",
    "    while loop_i < MAX_RESPONSE_LOOP and cons_score_knowledge < THRESHOLD_CONS:\n",
    "        if args.no_aspect:\n",
    "            instruction = f\"Please refine the response.\"\n",
    "        elif args.no_number:\n",
    "            instruction = f\"The alignment and consistency between the response and the context are low. Please refine the response to improve its consistency.\"\n",
    "        else:\n",
    "            instruction = f\"The consistency score for the response is {cons_score_knowledge} less than {THRESHOLD_CONS}, which means the alignment and consistency between the response and the context are low. Please refine the response to improve its consistency.\"\n",
    "\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are an AI language model designed to provide accurate, relevant, and comprehensive answers to questions based on the given background context.\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\", \n",
    "                \"content\": f\"The generated response for the question: \\\"{question}\\\" is \\\"{response}\\\" based on the background context: \\n{final_knowledge}.\\n\\n{instruction}\"\n",
    "            },\n",
    "        ]\n",
    "        response = generate_step(args, model, tokenizer, messages)\n",
    "\n",
    "        entailment_score_question, cons_score_knowledge = evaluate_response(entailment_scorer, ctrleval_scorer, question, response, final_knowledge)\n",
    "        candidates.append([(entailment_score_question + cons_score_knowledge) / 2, response])\n",
    "        entailment_score_question_list.append(entailment_score_question)\n",
    "        history.append([loop_i, response, entailment_score_question, cons_score_knowledge])\n",
    "\n",
    "        loop_i += 1\n",
    "\n",
    "    if MAX_RESPONSE_LOOP > 1 and cons_score_knowledge < THRESHOLD_CONS:\n",
    "        merge = zip(candidates, entailment_score_question_list)\n",
    "        merge = sorted(merge)\n",
    "        candidates, entailment_score_question_list = zip(*merge)\n",
    "        return candidates[-1][-1], history, entailment_score_question_list[-1]\n",
    "    else:\n",
    "        return response, history, entailment_score_question\n",
    "\n",
    "\n",
    "def reflection_loop(\n",
    "    args: Args,\n",
    "    question: str,\n",
    "    retrieved_docs: List[str],\n",
    ") -> Tuple[str, str, List[Tuple[int, str, float, float]]]:\n",
    "    all_history_response = []\n",
    "\n",
    "    THRESHOLD_ENTAIL = args.threshold_entailment\n",
    "    MAX_LOOP = args.max_loop\n",
    "\n",
    "    candidates = []\n",
    "    main_loop_i = 0\n",
    "    print(f\"main_loop {main_loop_i}\")\n",
    "\n",
    "    retrieved_context = \"\\n\".join([\"- \" + doc for doc in retrieved_docs])\n",
    "\n",
    "    final_response, history_response, entailment_score_question = response_loop(args, question, retrieved_context)\n",
    "    all_history_response += history_response\n",
    "    candidates.append([entailment_score_question, final_response])\n",
    "\n",
    "    main_loop_i += 1\n",
    "    while main_loop_i < MAX_LOOP and entailment_score_question < THRESHOLD_ENTAIL:\n",
    "        print(f\"main_loop {main_loop_i}\")\n",
    "\n",
    "        final_response, history_response, entailment_score_question = response_loop(args, question, retrieved_context)\n",
    "        all_history_response += history_response\n",
    "        candidates.append([entailment_score_question, final_response])\n",
    "        main_loop_i += 1\n",
    "\n",
    "    if (MAX_LOOP > 1) and entailment_score_question < THRESHOLD_ENTAIL:\n",
    "        candidates.sort()\n",
    "        final_response = candidates[-1][1]\n",
    "\n",
    "    return final_response, all_history_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "407e37ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_with_rag_reflection(\n",
    "    args: Args,\n",
    "    question: str,\n",
    "    knowledge_index: FAISS,\n",
    "    use_reranker: bool = True,\n",
    "    num_retrieved_docs: int = 30,\n",
    "    num_docs_final: int = 7,\n",
    ") -> Tuple[str, List[str]]:\n",
    "    \"\"\"RAG with self-reflection\"\"\"\n",
    "\n",
    "    # Step 1: Retrieve documents\n",
    "    print(\"=> Retrieving documents...\")\n",
    "    relevant_docs = knowledge_index.similarity_search(query=question, k=num_retrieved_docs)\n",
    "    relevant_docs = [doc.page_content for doc in relevant_docs]\n",
    "\n",
    "    # Step 2: Rerank if available\n",
    "    if use_reranker:\n",
    "        print(\"=> Reranking documents...\")\n",
    "        relevant_docs = reranker.rerank(question, relevant_docs, k=num_docs_final)\n",
    "        relevant_docs = [doc[\"content\"] for doc in relevant_docs]\n",
    "\n",
    "    relevant_docs = relevant_docs[:num_docs_final]\n",
    "\n",
    "    # Step 3: Generate response with self-reflection\n",
    "    print(\"=> Generating response with self-reflection...\")\n",
    "    final_response, _, = reflection_loop(args, question, relevant_docs)\n",
    "\n",
    "    return final_response, relevant_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5360ae60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 19.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating response with self-reflection...\n",
      "main_loop 0\n",
      "response_loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00,  6.04it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  5.96it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  5.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main_loop 1\n",
      "response_loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00,  6.11it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  5.91it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  5.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main_loop 2\n",
      "response_loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00,  6.27it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  5.94it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  5.88it/s]\n",
      " 50%|█████     | 1/2 [09:00<09:00, 540.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 26.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating response with self-reflection...\n",
      "main_loop 0\n",
      "response_loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00,  8.96it/s]\n",
      "100%|██████████| 2/2 [09:37<00:00, 288.73s/it]\n"
     ]
    }
   ],
   "source": [
    "num_samples = 2\n",
    "dataset = []\n",
    "for d in tqdm(ds.select(range(num_samples)), total=num_samples):\n",
    "    question = d[\"question\"]\n",
    "    reference = d[\"response\"]\n",
    "\n",
    "    final_response, relevant_docs = answer_with_rag_reflection(args, question, KNOWLEDGE_VECTOR_DATABASE, use_reranker=True)\n",
    "\n",
    "    dataset.append(\n",
    "        {\n",
    "            \"user_input\": question,\n",
    "            \"retrieved_contexts\": relevant_docs,\n",
    "            \"response\": final_response,\n",
    "            \"reference\": reference,\n",
    "            \"adherence_score\": d[\"adherence_score\"],\n",
    "            \"relevance_score\": d[\"relevance_score\"],\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "daab644a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1761240054.931996 2647483 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n",
      "/tmp/ipykernel_2647483/2935404805.py:1: DeprecationWarning: LangchainLLMWrapper is deprecated and will be removed in a future version. Use the modern LLM providers instead: from ragas.llms.base import llm_factory; llm = llm_factory('gpt-4o-mini') or from ragas.llms.base import instructor_llm_factory; llm = instructor_llm_factory('openai', client=openai_client)\n",
      "  evaluator_llm = LangchainLLMWrapper(\n"
     ]
    }
   ],
   "source": [
    "evaluator_llm = LangchainLLMWrapper(\n",
    "    ChatGoogleGenerativeAI(\n",
    "        model=\"gemini-2.5-flash\",\n",
    "        temperature=0,\n",
    "        max_tokens=None,\n",
    "        timeout=None,\n",
    "        max_retries=2,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4edc92d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "848860cc65e9425081b7137c8574659b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0000 00:00:1761240058.262086 2647483 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'factual_correctness(mode=f1)': 0.4300, 'faithfulness': 0.8295, 'nv_context_relevance': 0.6250}\n"
     ]
    }
   ],
   "source": [
    "evaluation_dataset = Dataset.from_list(dataset)\n",
    "\n",
    "ragas_result = evaluate(evaluation_dataset, metrics=[FactualCorrectness(), Faithfulness(), ContextRelevance()], llm=evaluator_llm)\n",
    "print(ragas_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1093f20e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_input</th>\n",
       "      <th>retrieved_contexts</th>\n",
       "      <th>response</th>\n",
       "      <th>reference</th>\n",
       "      <th>factual_correctness(mode=f1)</th>\n",
       "      <th>faithfulness</th>\n",
       "      <th>nv_context_relevance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Which university did one of the key figures in...</td>\n",
       "      <td>[Out to Win is an American documentary film, r...</td>\n",
       "      <td>Certainly. Below is a **refined, consistent, a...</td>\n",
       "      <td>One of the key figures in the American documen...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Were both Léopold Eyharts and Ulrich Walter a ...</td>\n",
       "      <td>[Léopold Eyharts (born April 28, 1957) is a Br...</td>\n",
       "      <td>No, both Léopold Eyharts and Ulrich Walter wer...</td>\n",
       "      <td>No, only Léopold Eyharts was a General in the ...</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          user_input  \\\n",
       "0  Which university did one of the key figures in...   \n",
       "1  Were both Léopold Eyharts and Ulrich Walter a ...   \n",
       "\n",
       "                                  retrieved_contexts  \\\n",
       "0  [Out to Win is an American documentary film, r...   \n",
       "1  [Léopold Eyharts (born April 28, 1957) is a Br...   \n",
       "\n",
       "                                            response  \\\n",
       "0  Certainly. Below is a **refined, consistent, a...   \n",
       "1  No, both Léopold Eyharts and Ulrich Walter wer...   \n",
       "\n",
       "                                           reference  \\\n",
       "0  One of the key figures in the American documen...   \n",
       "1  No, only Léopold Eyharts was a General in the ...   \n",
       "\n",
       "   factual_correctness(mode=f1)  faithfulness  nv_context_relevance  \n",
       "0                          0.00      0.750000                  0.50  \n",
       "1                          0.86      0.909091                  0.75  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ragas_result_df = ragas_result.to_pandas()\n",
    "ragas_result_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0fe72bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"ragbench/ragbench\")\n",
    "\n",
    "from evaluation import calculate_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "12b5eb5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['user_input', 'retrieved_contexts', 'response', 'reference', 'adherence_score', 'relevance_score', 'faithfulness', 'context_relevance'],\n",
       "    num_rows: 2\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation_dataset = evaluation_dataset.add_column(\"faithfulness\", ragas_result[\"faithfulness\"])\n",
    "evaluation_dataset = evaluation_dataset.add_column(\"context_relevance\", ragas_result[\"nv_context_relevance\"])\n",
    "evaluation_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ce80d476",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hallucination_auroc': 1.0, 'relevance_rmse': np.float64(0.4615988003900188)}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics = calculate_metrics(\n",
    "    evaluation_dataset,\n",
    "    pred_adherence=\"faithfulness\",  # adherence_score\n",
    "    pred_context_relevance=\"context_relevance\",  # relevance_score\n",
    ")\n",
    "metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI6130",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
