{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7f16f63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4042223/3106111402.py:17: UserWarning: \n",
      "********************************************************************************\n",
      "RAGatouille WARNING: Future Release Notice\n",
      "--------------------------------------------\n",
      "RAGatouille version 0.0.10 will be migrating to a PyLate backend \n",
      "instead of the current Stanford ColBERT backend.\n",
      "PyLate is a fully mature, feature-equivalent backend, that greatly facilitates compatibility.\n",
      "However, please pin version <0.0.10 if you require the Stanford ColBERT backend.\n",
      "********************************************************************************\n",
      "  from ragatouille import RAGPretrainedModel\n",
      "/data/hanchong/miniconda3/envs/AI6130/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'validate_default' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'validate_default' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv(\".env\")\n",
    "\n",
    "import os\n",
    "from typing import List, Tuple, Union\n",
    "\n",
    "import torch\n",
    "import json\n",
    "from datasets import Dataset, load_dataset\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.vectorstores.utils import DistanceStrategy\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from ragas import evaluate\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.metrics import ContextRelevance, FactualCorrectness, Faithfulness\n",
    "from ragatouille import RAGPretrainedModel\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4586f874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1883\n",
      "424\n",
      "390\n"
     ]
    }
   ],
   "source": [
    "ds_name = \"hotpotqa\"\n",
    "#ds_name = \"pubmedqa\"\n",
    "#ds_name = \"delucionqa\"\n",
    "ds = load_dataset(\"rungalileo/ragbench\", ds_name)\n",
    "print(len(ds[\"train\"]))\n",
    "print(len(ds[\"validation\"]))\n",
    "print(len(ds[\"test\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ed1523c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading existing knowledge vector database...\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_MODEL_NAME = \"Qwen/Qwen3-Embedding-0.6B\"\n",
    "\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=EMBEDDING_MODEL_NAME,\n",
    "    multi_process=False,\n",
    "    model_kwargs={\"device\": \"cuda\"},\n",
    "    encode_kwargs={\"normalize_embeddings\": True},  # Set `True` for cosine similarity\n",
    ")\n",
    "\n",
    "KNOWLEDGE_VECTOR_DB_PATH = f\"vector_store/{EMBEDDING_MODEL_NAME.replace('/', '~')}_{ds_name}\"\n",
    "\n",
    "if os.path.isdir(KNOWLEDGE_VECTOR_DB_PATH):\n",
    "    print(\"Loading existing knowledge vector database...\")\n",
    "    KNOWLEDGE_VECTOR_DATABASE = FAISS.load_local(\n",
    "        KNOWLEDGE_VECTOR_DB_PATH,\n",
    "        embedding_model,\n",
    "        allow_dangerous_deserialization=True,\n",
    "        distance_strategy=DistanceStrategy.COSINE,\n",
    "    )\n",
    "\n",
    "else:\n",
    "    RAW_KNOWLEDGE_BASE = []\n",
    "\n",
    "    for split in ds:\n",
    "        for d in ds[split]:\n",
    "            for doc in d[\"documents\"]:\n",
    "                RAW_KNOWLEDGE_BASE.append(doc)\n",
    "\n",
    "    RAW_KNOWLEDGE_BASE = list(set(RAW_KNOWLEDGE_BASE))\n",
    "    print(f\"Number of documents in knowledge base: {len(RAW_KNOWLEDGE_BASE)}\")\n",
    "\n",
    "    print(\"Creating knowledge vector database...\")\n",
    "    KNOWLEDGE_VECTOR_DATABASE = FAISS.from_texts(RAW_KNOWLEDGE_BASE, embedding_model, distance_strategy=DistanceStrategy.COSINE)\n",
    "    KNOWLEDGE_VECTOR_DATABASE.save_local(KNOWLEDGE_VECTOR_DB_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1508a6a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "603e1e2237cf4b45811dfcb749b9dd29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/hanchong/miniconda3/envs/AI6130/lib/python3.12/site-packages/colbert/utils/amp.py:12: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler()\n"
     ]
    }
   ],
   "source": [
    "READER_MODEL_NAME = \"Qwen/Qwen3-4B-Instruct-2507\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(READER_MODEL_NAME, device_map=\"cuda\", dtype=torch.bfloat16).eval()\n",
    "tokenizer = AutoTokenizer.from_pretrained(READER_MODEL_NAME)\n",
    "\n",
    "RERANKER = RAGPretrainedModel.from_pretrained(\"colbert-ir/colbertv2.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "963d3a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_step(\n",
    "    model: AutoModelForCausalLM,\n",
    "    tokenizer: AutoTokenizer,\n",
    "    messages: List[dict[str, str]],\n",
    "    max_new_tokens: int = 512,\n",
    "    temperature: float = 0.2,\n",
    "    top_p: float = 0.9,\n",
    "    top_k: int = 50,\n",
    "    num_beams: int = 1,\n",
    "    repetition_penalty: float = 1.1,\n",
    "    dola_decoding: bool = False,\n",
    "    activation_dola_decoding: bool = False,\n",
    "    dola_layers: Union[str, list[int]] = \"high\",\n",
    "    sled_decoding: bool = False,\n",
    "    activation_sled_decoding: bool = False,\n",
    "    end_sled_decoding: bool = False,\n",
    "    evolution_rate: float = 2.0,\n",
    "    evolution_scale: int = 10,\n",
    "    evolution_lower_bound: float = -1000.0,\n",
    ") -> str:\n",
    "    formatted_chat = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    inputs = tokenizer(formatted_chat, return_tensors=\"pt\", add_special_tokens=False).to(model.device)\n",
    "    if dola_decoding:\n",
    "        print(\"=> Using DOLA decoding...\")\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=temperature > 0,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            top_k=top_k,\n",
    "            num_beams=num_beams,\n",
    "            repetition_penalty=repetition_penalty,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            custom_generate=\"custom_decoding/dola\",\n",
    "            trust_remote_code=True,\n",
    "            dola_layers=dola_layers,\n",
    "        )\n",
    "    elif activation_dola_decoding:\n",
    "        print(\"=> Using Activation DOLA decoding...\")\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=temperature > 0,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            top_k=top_k,\n",
    "            num_beams=num_beams,\n",
    "            repetition_penalty=repetition_penalty,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            custom_generate=\"custom_decoding/activation_dola\",\n",
    "            trust_remote_code=True,\n",
    "            dola_layers=dola_layers,\n",
    "            alpha=0.5,\n",
    "        )\n",
    "    elif sled_decoding:\n",
    "        print(\"=> Using SLED decoding...\")\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=temperature > 0,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            top_k=top_k,\n",
    "            num_beams=num_beams,\n",
    "            repetition_penalty=repetition_penalty,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            custom_generate=\"custom_decoding/sled\",\n",
    "            trust_remote_code=True,\n",
    "            evolution_rate=evolution_rate,\n",
    "            evolution_scale=evolution_scale,\n",
    "            evolution_lower_bound=evolution_lower_bound,\n",
    "        )\n",
    "    elif activation_sled_decoding:\n",
    "        print(\"=> Using Activation SLED decoding...\")\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=temperature > 0,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            top_k=top_k,\n",
    "            num_beams=num_beams,\n",
    "            repetition_penalty=repetition_penalty,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            custom_generate=\"custom_decoding/activation_sled\",\n",
    "            trust_remote_code=True,\n",
    "            evolution_rate=evolution_rate,\n",
    "            evolution_scale=evolution_scale,\n",
    "            evolution_lower_bound=evolution_lower_bound,\n",
    "            alpha=0.5,\n",
    "        )\n",
    "    elif end_sled_decoding:\n",
    "        print(\"=> Using End-SLED decoding...\")\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=temperature > 0,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            top_k=top_k,\n",
    "            num_beams=num_beams,\n",
    "            repetition_penalty=repetition_penalty,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            custom_generate=\"custom_decoding/end_sled\",\n",
    "            trust_remote_code=True,\n",
    "            evolution_rate=evolution_rate,\n",
    "            evolution_scale=evolution_scale,\n",
    "            evolution_lower_bound=evolution_lower_bound,\n",
    "            alpha=0.5,\n",
    "        )\n",
    "    else:\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=temperature > 0,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            top_k=top_k,\n",
    "            num_beams=num_beams,\n",
    "            repetition_penalty=repetition_penalty,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    return tokenizer.decode(outputs[0][inputs[\"input_ids\"].size(1) :], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "def answer_with_rag(\n",
    "    question: str,\n",
    "    use_reranker: bool = True,\n",
    "    num_retrieved_docs: int = 50,\n",
    "    num_docs_final: int = 10,\n",
    "    dola_decoding=False,\n",
    "    activation_dola_decoding=False,\n",
    "    sled_decoding=False,\n",
    "    activation_sled_decoding=False,\n",
    "    end_sled_decoding=False,\n",
    ") -> Tuple[str, List[str]]:\n",
    "    print(\"=> Retrieving documents...\")\n",
    "    relevant_docs = KNOWLEDGE_VECTOR_DATABASE.similarity_search(query=question, k=num_retrieved_docs)\n",
    "    relevant_docs = [doc.page_content for doc in relevant_docs]  # Keep only the text\n",
    "\n",
    "    if use_reranker:\n",
    "        print(\"=> Reranking documents...\")\n",
    "        relevant_docs = RERANKER.rerank(question, relevant_docs, k=num_docs_final)\n",
    "        relevant_docs = [doc[\"content\"] for doc in relevant_docs]\n",
    "\n",
    "    relevant_docs = relevant_docs[:num_docs_final]\n",
    "\n",
    "    print(\"=> Generating answer...\")\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"\"\"You are a chatbot providing answers to user queries. You will be given one or more context documents, and a question. \\\n",
    "Use the information in the documents to answer the question.\n",
    "\n",
    "If the documents do not provide enough information to answer the question, provide the best possible answer based on your existing knowledge. \\\n",
    "Do not make up facts or provide incorrect information.\"\"\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"\"\"Answer the question in one paragraph using the provided context.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\"\"\".format(\n",
    "                context=\"\\n\".join([\"- \" + doc for doc in relevant_docs]),\n",
    "                question=question,\n",
    "            ),\n",
    "        },\n",
    "    ]\n",
    "    response = generate_step(\n",
    "        model,\n",
    "        tokenizer,\n",
    "        messages,\n",
    "        max_new_tokens=512,\n",
    "        temperature=0.2,\n",
    "        repetition_penalty=1.2 if dola_decoding or sled_decoding else 1.1,\n",
    "        dola_decoding=dola_decoding,\n",
    "        activation_dola_decoding=activation_dola_decoding,\n",
    "        sled_decoding=sled_decoding,\n",
    "        activation_sled_decoding=activation_sled_decoding,\n",
    "        end_sled_decoding=end_sled_decoding,\n",
    "    )\n",
    "    return response, relevant_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "864f16b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/hanchong/miniconda3/envs/AI6130/lib/python3.12/site-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 28.68it/s]\n",
      "`repetition_penalty` is set to a value of 1.1, which could induce unwanted repetition. The recommended value for SELD decoding is `repetition_penalty>=1.2`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating answer...\n",
      "=> Using End-SLED decoding...\n",
      "0.5\n",
      "None of the key figures mentioned in the documentary *Out to Win* (directed by Malcolm Ingram) are stated to have attended a specific university prior to being drafted by the New Jersey Nets. Additionally, there is no mention of any individual being selected 18th overall by the New Jersey Nets in the provided context. Therefore, based on the available information, it is impossible to determine which university one of these individuals paid for before being drafted. The reference to the New Jersey Nets drafting someone 18th overall does not align with any factual detail in the text regarding the film's subjects. Hence, the question cannot be answered with certainty using the provided context.\n"
     ]
    }
   ],
   "source": [
    "response, relevant_docs = answer_with_rag(ds[\"test\"][0][\"question\"], use_reranker=True, end_sled_decoding=True)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821849cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 100\n",
    "dataset = []\n",
    "for d in tqdm(ds[\"test\"].select(range(num_samples)), total=num_samples, desc=\"Processing test samples\"):\n",
    "    question = d[\"question\"]\n",
    "    reference = d[\"response\"]\n",
    "    response, relevant_docs = answer_with_rag(question, use_reranker=True, dola_decoding=True)\n",
    "\n",
    "    dataset.append(\n",
    "        {\n",
    "            \"user_input\": question,\n",
    "            \"retrieved_contexts\": relevant_docs,\n",
    "            \"response\": response,\n",
    "            \"reference\": reference,\n",
    "            \"adherence_score\": d[\"adherence_score\"],\n",
    "            \"relevance_score\": d[\"relevance_score\"],\n",
    "        }\n",
    "    )\n",
    "\n",
    "output_dir = \"results/exp-2\"\n",
    "with open(os.path.join(output_dir, f\"{READER_MODEL_NAME.replace('/', '~')}_{ds_name}_rag-responses.json\"), \"w\") as f:\n",
    "    json.dump(dataset, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4dbfa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator_llm = LangchainLLMWrapper(\n",
    "    ChatGoogleGenerativeAI(\n",
    "        model=\"gemini-2.5-flash-lite\",\n",
    "        temperature=0.1,\n",
    "        max_tokens=None,\n",
    "        timeout=None,\n",
    "        max_retries=4,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933102fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_dataset = Dataset.from_list(dataset)\n",
    "\n",
    "ragas_result = evaluate(evaluation_dataset, metrics=[FactualCorrectness(), Faithfulness(), ContextRelevance()], llm=evaluator_llm)\n",
    "print(ragas_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58fbc6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ragas_result_df = ragas_result.to_pandas()\n",
    "ragas_result_df.to_csv(os.path.join(output_dir, f\"{READER_MODEL_NAME.replace('/', '~')}_{ds_name}_ragas-results.csv\"), index=False)\n",
    "#ragas_result_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629989c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"ragbench/ragbench\")\n",
    "\n",
    "from evaluation import calculate_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3641f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_dataset = evaluation_dataset.add_column(\"faithfulness\", ragas_result[\"faithfulness\"])\n",
    "evaluation_dataset = evaluation_dataset.add_column(\"context_relevance\", ragas_result[\"nv_context_relevance\"])\n",
    "evaluation_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f7b150",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = calculate_metrics(\n",
    "    evaluation_dataset,\n",
    "    pred_adherence=\"faithfulness\",  # adherence_score\n",
    "    pred_context_relevance=\"context_relevance\",  # relevance_score\n",
    ")\n",
    "print(metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI6130",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
