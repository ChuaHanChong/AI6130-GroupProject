{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7f16f63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3509902/1110040595.py:17: UserWarning: \n",
      "********************************************************************************\n",
      "RAGatouille WARNING: Future Release Notice\n",
      "--------------------------------------------\n",
      "RAGatouille version 0.0.10 will be migrating to a PyLate backend \n",
      "instead of the current Stanford ColBERT backend.\n",
      "PyLate is a fully mature, feature-equivalent backend, that greatly facilitates compatibility.\n",
      "However, please pin version <0.0.10 if you require the Stanford ColBERT backend.\n",
      "********************************************************************************\n",
      "  from ragatouille import RAGPretrainedModel\n",
      "/data/hanchong/miniconda3/envs/AI6130/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'validate_default' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'validate_default' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv(\".env\")\n",
    "\n",
    "import os\n",
    "from typing import List, Optional, Tuple\n",
    "\n",
    "import torch\n",
    "import json\n",
    "from datasets import Dataset, load_dataset\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.vectorstores.utils import DistanceStrategy\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from ragas import evaluate\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.metrics import ContextRelevance, FactualCorrectness, Faithfulness\n",
    "from ragatouille import RAGPretrainedModel\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, Pipeline, pipeline\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4586f874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1883\n",
      "424\n",
      "390\n"
     ]
    }
   ],
   "source": [
    "ds_name = \"hotpotqa\"\n",
    "#ds_name = \"pubmedqa\"\n",
    "#ds_name = \"delucionqa\"\n",
    "ds = load_dataset(\"rungalileo/ragbench\", ds_name)\n",
    "print(len(ds['train']))\n",
    "print(len(ds['validation']))\n",
    "print(len(ds['test']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ed1523c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading existing knowledge vector database...\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_MODEL_NAME = \"Qwen/Qwen3-Embedding-0.6B\"\n",
    "\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=EMBEDDING_MODEL_NAME,\n",
    "    multi_process=False,\n",
    "    model_kwargs={\"device\": \"cuda\"},\n",
    "    encode_kwargs={\"normalize_embeddings\": True},  # Set `True` for cosine similarity\n",
    ")\n",
    "\n",
    "KNOWLEDGE_VECTOR_DB_PATH = f\"vector_store/{EMBEDDING_MODEL_NAME.replace('/', '~')}_{ds_name}\"\n",
    "\n",
    "if os.path.isdir(KNOWLEDGE_VECTOR_DB_PATH):\n",
    "    print(\"Loading existing knowledge vector database...\")\n",
    "    KNOWLEDGE_VECTOR_DATABASE = FAISS.load_local(\n",
    "        KNOWLEDGE_VECTOR_DB_PATH, \n",
    "        embedding_model, \n",
    "        allow_dangerous_deserialization=True,\n",
    "        distance_strategy=DistanceStrategy.COSINE,\n",
    "    )\n",
    "\n",
    "else:\n",
    "    RAW_KNOWLEDGE_BASE = []\n",
    "\n",
    "    for split in ds:\n",
    "        for d in ds[split]:\n",
    "            for doc in d[\"documents\"]:\n",
    "                RAW_KNOWLEDGE_BASE.append(doc)\n",
    "\n",
    "    RAW_KNOWLEDGE_BASE = list(set(RAW_KNOWLEDGE_BASE))\n",
    "    print(f\"Number of documents in knowledge base: {len(RAW_KNOWLEDGE_BASE)}\")\n",
    "\n",
    "    print(\"Creating knowledge vector database...\")\n",
    "    KNOWLEDGE_VECTOR_DATABASE = FAISS.from_texts(RAW_KNOWLEDGE_BASE, embedding_model, distance_strategy=DistanceStrategy.COSINE)\n",
    "    KNOWLEDGE_VECTOR_DATABASE.save_local(KNOWLEDGE_VECTOR_DB_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1508a6a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31032ac9d568467ba3a79e2df340d8a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda\n"
     ]
    }
   ],
   "source": [
    "READER_MODEL_NAME = \"Qwen/Qwen3-4B-Instruct-2507\"\n",
    "#READER_MODEL_NAME = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(READER_MODEL_NAME, device_map=\"cuda\", dtype=torch.bfloat16).eval()\n",
    "tokenizer = AutoTokenizer.from_pretrained(READER_MODEL_NAME)\n",
    "\n",
    "READER_LLM = pipeline(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    task=\"text-generation\",\n",
    "    do_sample=True,\n",
    "    temperature=0.2,\n",
    "    repetition_penalty=1.1,\n",
    "    return_full_text=False,\n",
    "    max_new_tokens=512,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b26878c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are a chatbot providing answers to user queries. You will be given one or more context documents, and a question. Use the information in the documents to answer the question.\n",
      "\n",
      "If the documents do not provide enough information for you to answer the question, then say \"The documents are missing some of the information required to answer the question.\" Don't quote any external knowledge that is not in the documents. Don't try to make up an answer.<|im_end|>\n",
      "<|im_start|>user\n",
      "Answer the question using the provided context.\n",
      "\n",
      "Context:\n",
      "{context}\n",
      "\n",
      "Question: {question}<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt_in_chat_format = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"\"\"You are a chatbot providing answers to user queries. You will be given one or more context documents, and a question. \\\n",
    "Use the information in the documents to answer the question.\n",
    "\n",
    "If the documents do not provide enough information for you to answer the question, then say \\\n",
    "\"The documents are missing some of the information required to answer the question.\" Don't quote any external knowledge that is \\\n",
    "not in the documents. Don't try to make up an answer.\"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"\"\"Answer the question using the provided context.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\"\"\",\n",
    "    },\n",
    "]\n",
    "RAG_PROMPT_TEMPLATE = tokenizer.apply_chat_template(prompt_in_chat_format, tokenize=False, add_generation_prompt=True)\n",
    "print(RAG_PROMPT_TEMPLATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6afa398a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/hanchong/miniconda3/envs/AI6130/lib/python3.12/site-packages/colbert/utils/amp.py:12: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler()\n"
     ]
    }
   ],
   "source": [
    "RERANKER = RAGPretrainedModel.from_pretrained(\"colbert-ir/colbertv2.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963d3a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_with_rag(\n",
    "    question: str,\n",
    "    llm: Pipeline,\n",
    "    knowledge_index: FAISS,\n",
    "    reranker: Optional[RAGPretrainedModel] = None,\n",
    "    num_retrieved_docs: int = 50,\n",
    "    num_docs_final: int = 10,\n",
    ") -> Tuple[str, List[str]]:\n",
    "    print(\"=> Retrieving documents...\")\n",
    "    relevant_docs = knowledge_index.similarity_search(query=question, k=num_retrieved_docs)\n",
    "    relevant_docs = [doc.page_content for doc in relevant_docs]  # Keep only the text\n",
    "\n",
    "    if reranker:\n",
    "        print(\"=> Reranking documents...\")\n",
    "        relevant_docs = reranker.rerank(question, relevant_docs, k=num_docs_final)\n",
    "        relevant_docs = [doc[\"content\"] for doc in relevant_docs]\n",
    "\n",
    "    relevant_docs = relevant_docs[:num_docs_final]\n",
    "\n",
    "    final_prompt = RAG_PROMPT_TEMPLATE.format(\n",
    "        question=question, context=\"\\n\".join([\"- \" + doc for doc in relevant_docs])\n",
    "    )\n",
    "\n",
    "    print(\"=> Generating answer...\")\n",
    "    response = llm(final_prompt)[0][\"generated_text\"]\n",
    "    return response, relevant_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821849cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 100\n",
    "dataset = []\n",
    "for d in tqdm(ds['test'].select(range(num_samples)), total=num_samples, desc=\"Processing test samples\"):\n",
    "    question = d[\"question\"]\n",
    "    reference = d[\"response\"]\n",
    "    response, relevant_docs = answer_with_rag(question, READER_LLM, KNOWLEDGE_VECTOR_DATABASE, reranker=RERANKER)\n",
    "\n",
    "    # relevant_docs = d['documents']  # For testing purpose, use reference docs as retrieved docs\n",
    "    # response = reference  # For testing purpose, use reference as response\n",
    "\n",
    "    dataset.append(\n",
    "        {\n",
    "            \"user_input\": question,\n",
    "            \"retrieved_contexts\": relevant_docs,\n",
    "            \"response\": response,\n",
    "            \"reference\": reference,\n",
    "            \"adherence_score\": d[\"adherence_score\"],\n",
    "            \"relevance_score\": d[\"relevance_score\"],\n",
    "        }\n",
    "    )\n",
    "\n",
    "output_dir = \"results/exp-1\"\n",
    "with open(os.path.join(output_dir, f\"{READER_MODEL_NAME.replace('/', '~')}_{ds_name}_rag-responses.json\"), \"w\") as f:\n",
    "    json.dump(dataset, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4dbfa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_huggingface import HuggingFacePipeline\n",
    "\n",
    "# model_id = \"Qwen/Qwen3-4B-Instruct-2507\"\n",
    "# evaluator_llm = LangchainLLMWrapper(\n",
    "#    HuggingFacePipeline(\n",
    "#        pipeline=pipeline(\n",
    "#            \"text-generation\",\n",
    "#            model=AutoModelForCausalLM.from_pretrained(model_id),\n",
    "#            tokenizer=AutoTokenizer.from_pretrained(model_id),\n",
    "#            device_map=\"auto\",\n",
    "#            max_new_tokens=32,\n",
    "#        )\n",
    "#    )\n",
    "# )\n",
    "\n",
    "\n",
    "evaluator_llm = LangchainLLMWrapper(\n",
    "    ChatGoogleGenerativeAI(\n",
    "        model=\"gemini-2.5-flash-lite\",\n",
    "        temperature=0.1,\n",
    "        max_tokens=None,\n",
    "        timeout=None,\n",
    "        max_retries=4,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933102fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_dataset = Dataset.from_list(dataset)\n",
    "\n",
    "ragas_result = evaluate(evaluation_dataset, metrics=[FactualCorrectness(), Faithfulness(), ContextRelevance()], llm=evaluator_llm)\n",
    "print(ragas_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58fbc6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ragas_result_df = ragas_result.to_pandas()\n",
    "ragas_result_df.to_csv(os.path.join(output_dir, f\"{READER_MODEL_NAME.replace('/', '~')}_{ds_name}_ragas-results.csv\"), index=False)\n",
    "#ragas_result_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629989c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"ragbench/ragbench\")\n",
    "\n",
    "from evaluation import calculate_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3641f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_dataset = evaluation_dataset.add_column(\"faithfulness\", ragas_result[\"faithfulness\"])\n",
    "evaluation_dataset = evaluation_dataset.add_column(\"context_relevance\", ragas_result[\"nv_context_relevance\"])\n",
    "evaluation_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f7b150",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = calculate_metrics(\n",
    "    evaluation_dataset,\n",
    "    pred_adherence=\"faithfulness\",  # adherence_score\n",
    "    pred_context_relevance=\"context_relevance\",  # relevance_score\n",
    ")\n",
    "print(metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI6130",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
